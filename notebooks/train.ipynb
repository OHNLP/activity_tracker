{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load features\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datajoint as dj\n",
    "from activity_tracker.pipeline import subject, models, ingestion\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import pandas as pd\n",
    "df = models.Feature.get_feature_matrix(1)\n",
    "df.drop(columns=['group', 'race', 'total_steps_std', 'interval_length'], inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute missing values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing monthly_income with median\n",
    "median_income = df['monthly_income'].median()\n",
    "df['monthly_income'] = df['monthly_income'].fillna(median_income)\n",
    "\n",
    "# Fill missing education with mode\n",
    "df['education'] = df['education'].fillna(df[\"education\"].mode().iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check missing values\n",
    "total_rows = len(df)\n",
    "missing_summary = df.isnull().sum().to_frame(name='missing_count')\n",
    "missing_summary['missing_percent'] = (missing_summary['missing_count'] / total_rows * 100).round(2)\n",
    "missing_summary = missing_summary[missing_summary['missing_count'] > 0].sort_values(by='missing_count', ascending=False)\n",
    "\n",
    "print(missing_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import GroupShuffleSplit, cross_val_score\n",
    "\n",
    "# Prepare features and target\n",
    "X = df.drop(['subject_id', 'start_date', 'measurement_date', 'target_ffp_status'], axis=1)\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(df['target_ffp_status'])  # 0: frail, 1: no_frail\n",
    "\n",
    "# Encode categorical features\n",
    "for col in X.select_dtypes(include=['object']).columns:\n",
    "    X[col] = LabelEncoder().fit_transform(X[col].astype(str))\n",
    "\n",
    "# Model\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\n",
    "\n",
    "# FSubject-level splitting to prevent data leakage\n",
    "gss = GroupShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\n",
    "f1_macro_scores = cross_val_score(clf, X, y, cv=gss, scoring='f1_macro', groups=df['subject_id'])\n",
    "accuracy_scores = cross_val_score(clf, X, y, cv=gss, scoring='accuracy', groups=df['subject_id'])\n",
    "\n",
    "print(\"Randome Forest: 5-fold cross-validation\")\n",
    "print(\"-\"*50)\n",
    "print(\"F1 score:\", f1_macro_scores.mean().round(3))\n",
    "print(\"Accuracy:\", accuracy_scores.mean().round(3))\n",
    "\n",
    "# Per-class performance tracking with subject-level splitting\n",
    "reports = []\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(gss.split(X, y, groups=df['subject_id']), 1):\n",
    "    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "    \n",
    "    # Verify no subject overlap\n",
    "    train_subjects = set(df.iloc[train_idx]['subject_id'])\n",
    "    test_subjects = set(df.iloc[test_idx]['subject_id'])\n",
    "    overlap = len(train_subjects.intersection(test_subjects))\n",
    "    \n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    report = classification_report(y_test, y_pred, output_dict=True)\n",
    "    \n",
    "    # Rename class labels: 0 → frail, 1 → no_frail\n",
    "    df_report = pd.DataFrame(report).T.loc[['0', '1'], ['precision', 'recall', 'f1-score']]\n",
    "    df_report.index = df_report.index.map({'0': 'frail', '1': 'no_frail'})\n",
    "    df_report['fold'] = fold\n",
    "    reports.append(df_report)\n",
    "\n",
    "final_report_df = pd.concat(reports).reset_index().rename(columns={'index': 'class'})\n",
    "avg_metrics = final_report_df.groupby('class')[['precision', 'recall', 'f1-score']].mean().round(3)\n",
    "avg_metrics.loc['average'] = avg_metrics.mean()\n",
    "print(avg_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "# Fit on all data (or use the last fold's model)\n",
    "clf.fit(X, y)\n",
    "importances = clf.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "print(\"Feature ranking:\")\n",
    "for f in range(X.shape[1]):\n",
    "    print(f\"{f + 1}. {X.columns[indices[f]]} ({importances[indices[f]]:.4f})\")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.title(\"Feature Importance (from random forest)\")\n",
    "plt.bar(range(X.shape[1]), importances[indices], align=\"center\")\n",
    "plt.xticks(range(X.shape[1]), X.columns[indices], rotation=90)\n",
    "plt.tight_layout()\n",
    "sns.despine()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix\n",
    "corr = X.corr()\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(corr, annot=True, fmt=\".2f\", cmap=\"coolwarm\", square=True)\n",
    "plt.title(\"Feature Correlation Matrix\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "clf = xgb.XGBClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=3,\n",
    "    random_state=42,\n",
    "    scale_pos_weight=1,\n",
    "    # use_label_encoder=False,\n",
    "    eval_metric='logloss',\n",
    "    n_jobs=-1 \n",
    ")\n",
    "\n",
    "# Subject-level splitting to prevent data leakage\n",
    "gss = GroupShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\n",
    "f1_macro_scores = cross_val_score(clf, X, y, cv=gss, scoring='f1_macro', groups=df['subject_id'])\n",
    "accuracy_scores = cross_val_score(clf, X, y, cv=gss, scoring='accuracy', groups=df['subject_id'])\n",
    "\n",
    "print(\"XGBoost - 5-fold cross-validation\")\n",
    "print(\"-\"*50)\n",
    "print(\"F1 score:\", f1_macro_scores.mean().round(3))\n",
    "print(\"Accuracy:\", accuracy_scores.mean().round(3))\n",
    "\n",
    "reports = []\n",
    "for fold, (train_idx, test_idx) in enumerate(gss.split(X, y, groups=df['subject_id']), 1):\n",
    "    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "    \n",
    "    # Verify no subject overlap\n",
    "    train_subjects = set(df.iloc[train_idx]['subject_id'])\n",
    "    test_subjects = set(df.iloc[test_idx]['subject_id'])\n",
    "    overlap = len(train_subjects.intersection(test_subjects))\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    report = classification_report(y_test, y_pred, output_dict=True)\n",
    "    df_report = pd.DataFrame(report).T.loc[['0', '1'], ['precision', 'recall', 'f1-score']]\n",
    "    df_report.index = df_report.index.map({'0': 'frail', '1': 'no_frail'})\n",
    "    df_report['fold'] = fold\n",
    "    reports.append(df_report)\n",
    "\n",
    "final_report_df = pd.concat(reports).reset_index().rename(columns={'index': 'class'})\n",
    "avg_metrics = final_report_df.groupby('class')[['precision', 'recall', 'f1-score']].mean().round(3)\n",
    "avg_metrics.loc['average'] = avg_metrics.mean()\n",
    "print(avg_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "# Fit model\n",
    "clf = xgb.XGBClassifier(n_estimators=100, random_state=42, use_label_encoder=False, eval_metric='logloss')\n",
    "clf.fit(X, y)\n",
    "\n",
    "# Automatic feature selection\n",
    "selector = SelectFromModel(clf, threshold=\"median\", prefit=True)\n",
    "X_selected = selector.transform(X)\n",
    "selected_features = X.columns[selector.get_support()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "activity_tracker",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
